{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003d3879",
   "metadata": {},
   "source": [
    "# Подготовка модели распознавания рукописных букв и цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c990201-9fd1-47ed-92ae-fec393adaf82",
   "metadata": {},
   "source": [
    "Необходимо решить задачу классификации на основе датасета рукописных символов EMNIST и оформить модель как сервис. Таким образом, решение состоит из следующих шагов:\r\n",
    "\r\n",
    "Подготовка данных, построение, обучение и тестирование модели.\r\n",
    "Обёртка готовой модели в сервис, запуск веб-приложения в Docker-контейнере, подготовка репозитория.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cb7bbc2-eaec-4f6b-b98b-0644870faf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install tqdm\n",
    "#!pip install torch torchvision torchinfo\n",
    "#!pip install -U scikit-image matplotlib numpy\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239aaa14-3962-49d6-a4ac-faddfe175a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import emnist\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fa0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#датасет уже скачан и находится в директории data/\n",
    "#dataset = EMNIST('data/', 'balanced', download=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170974aa-54cc-4ad1-92f6-92452d56ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, labels_train = emnist.extract_training_samples('balanced')\n",
    "images_test, labels_test = emnist.extract_test_samples('balanced')\n",
    "\n",
    "#количество семплов в каждом сплите датасета и размер изображений\n",
    "print(f'Train: {len(images_train)} samples')\n",
    "print(f'Test: {len(images_test)} samples')\n",
    "print(f'Image size: {images_train[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7711da06-1ead-4448-9ac4-82bf10fe2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем словарь соответствий mapping\n",
    "with open('emnist-balanced-mapping.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "mapping = {int(line.split()[0]): chr(int(line.split()[1])) for line in lines}\n",
    "\n",
    "images_train, labels_train = emnist.extract_training_samples('balanced')\n",
    "\n",
    "characters = [mapping[label] for label in labels_train]\n",
    "\n",
    "char_counts = pd.Series(characters).value_counts()\n",
    "\n",
    "for label, symbol in mapping.items():\n",
    "    count = char_counts[symbol] if symbol in char_counts else 0\n",
    "    print(f\"label: {label}, Символ: {symbol}, Кол-во семплов: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3da80b-d0df-41df-aff9-aae625ea4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры изображения\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    image = images_train[i].reshape(28, 28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9d8d3-729d-4935-8be7-f64843431863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # редактирование ориентации (зеркализации) изображения\n",
    "# def display_image(image):\n",
    "#     # Проверка необходимой ориентации\n",
    "#     if len(image.shape) == 3:\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Загрузка и отображение изображения\n",
    "# image_path = 'path_to_your_image.png'\n",
    "# image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Если это черно-белое изображение\n",
    "# display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbe17e19-a052-49c1-9d9b-38930da4eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping сохраняем в ф-л mapping.pkl\n",
    "with open(os.path.join('myapp', 'mapping.pkl'),'wb') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "006f8b21-87fe-43ad-b636-5fc4418d1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим в тензоры\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    #Normalize([0.5], [0.5])\n",
    "    Normalize([0.1307], [0.3081])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.EMNIST('data/', split='balanced', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.EMNIST('data/', split='balanced', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fff7cc-bc7e-44dd-9f1e-53ced4d7092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv2d - это слой (layer) в библиотеке PyTorch 2D свёртки\n",
    "#параметры слоя Conv2d включают:\n",
    "# in_channels: количество каналов входящего изображения.\n",
    "# out_channels: количество выходных каналов (то есть количество ядер свёртки).\n",
    "# kernel_size: размер ядра свёртки (например, 3x3, 5x5 и т.д.).\n",
    "# stride: шаг свёртки (по умолчанию равен 1).\n",
    "# padding: тип паддинга (валидный или полный, по умолчанию \"валидный\").\n",
    "# dilation: значение разреженности (по умолчанию равно 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8cd43d86-df1d-4212-857f-8c52f5d741a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,  # свертка на вход приходит ч.б. изображение-1канал  (цветное 3канала)\n",
    "                      out_channels=32, \n",
    "                      kernel_size=3,\n",
    "                      padding=1),                   \n",
    "            nn.ReLU(),                  # делаем активацию функцией ReLU\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2), # MaxPool2d уменьшаем кол-во параметров (кол-во признаков/4) \n",
    "\n",
    "            # + слой Conv2d для дилатации dilation=2\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, dilation=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "            #nn.ReLU(),\n",
    "            #nn.AvgPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Flatten(),# вытягиваем в вектор\n",
    "# предиктор...слой 6272признаков (28х28)х32/4 (MaxPool2d) на вход\n",
    "            nn.Linear(in_features=6272, out_features=64), # задаем кол-во скрытых признаков out_features=128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128), # задаем кол-во скрытых признаков out_features=128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=n_classes) # предскажем кол-во классов out_features=n_classes net = CNN(47)\n",
    "         )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "758b4bcb-18cf-4bfc-9814-7c65d6766ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [1, 47]                   --\n",
       "├─Sequential: 1-1                        [1, 47]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 28, 28]           320\n",
       "│    └─ReLU: 2-2                         [1, 32, 28, 28]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 32, 14, 14]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 14, 14]           9,248\n",
       "│    └─ReLU: 2-5                         [1, 32, 14, 14]           --\n",
       "│    └─Flatten: 2-6                      [1, 6272]                 --\n",
       "│    └─Linear: 2-7                       [1, 64]                   401,472\n",
       "│    └─ReLU: 2-8                         [1, 64]                   --\n",
       "│    └─Linear: 2-9                       [1, 128]                  8,320\n",
       "│    └─ReLU: 2-10                        [1, 128]                  --\n",
       "│    └─Linear: 2-11                      [1, 47]                   6,063\n",
       "==========================================================================================\n",
       "Total params: 425,423\n",
       "Trainable params: 425,423\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 2.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.25\n",
       "Params size (MB): 1.70\n",
       "Estimated Total Size (MB): 1.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!!!!!!!!!!!эту ячейку надо запускать перед предиктором (закоментировать) тогда посчитает кол-во признаков 6272\n",
    "net = CNN(47)\n",
    "summary(net, input_size=(1, 1, 28, 28))# 1 - количество батчей (batch size), 1 - количество каналов (channels), 28х28 размер высота ширина\n",
    "#после повторного запуска видим в конце вектор из 47 предсказаний Linear: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4cb1a73c-6866-4479-9cf0-1d04c99c81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0\n",
    "def train(model, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre):\n",
    "    model.train()\n",
    "    for epoch in range(n_epoch):\n",
    "        loss_sum = 0\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for step, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze(1)\n",
    "            loss = loss_f(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f'Iter: {step} \\tLoss: {loss.item()}')\n",
    "\n",
    "        print(f'Mean Train Loss: {loss_sum / (step + 1):.6f}', end='\\n\\n')\n",
    "\n",
    "        if epoch % val_fre == 0:\n",
    "            validate(model, val_loader)\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data).squeeze(1)\n",
    "            loss = loss_f(output, target)\n",
    "            loss_sum += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Val Loss: {loss_sum / len(val_loader):.6f} \\tValidation Accuracy: {accuracy:.4f}')\n",
    "    # Обновление лучшей точности, если текущая точность выше\n",
    "    global best_val_accuracy\n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5b9f99-1daf-477b-9024-0088d0df8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(47)                # создаем модель 47 классов которые надо распознать (mapping) \n",
    "loss_f = nn.CrossEntropyLoss() # фиксируем Loss\n",
    "#loss_f = nn.NLLLoss() #  другие функции потерь\n",
    "#loss_f =nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-1) # применяем оптимизатор\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-5, amsgrad=True)\n",
    "\n",
    "n_epoch = 20 # кол-во эпох\n",
    "val_fre = 2  # как часто делаем валидацию\n",
    "\n",
    "train(model, optimizer, loss_f, train_loader, val_loader, n_epoch, val_fre)\n",
    "validate(model, val_loader)\n",
    "print(f'Accuracy: {best_val_accuracy}')\n",
    "# ошибки Val Loss: уменьшаются,  точность \tAccuracy: растет - модель обучается \n",
    "# (обучение может остановится и даже пойти переобучение Val Loss растет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "112ee911-2610-4f5d-84b4-5f492a12f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8707446808510638\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {best_val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c1d76-cfef-4cb5-8f3d-667e534a1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=1000             Val Loss: 0.471095 \tAccuracy: 0.8432978723404255\n",
    "# batch_size=512              Val Loss: 0.444851 \tAccuracy: 0.8554787234042553\n",
    "# batch_size=512 +1слойLinear_64 Val Loss: 0.441709 Accuracy: 0.8507446808510638\n",
    "# batch_size=512 + nn.NLLLoss Val Loss: nan \t    Accuracy: 0.02127659574468085\n",
    "# batch_size=256 features=64  Val Loss: 0.565492 \tAccuracy: 0.8465425531914894\n",
    "# batch_size=512 Normalize([0.1307], [0.3081]) Val Loss: 0.469646 \tAccuracy: 0.8529787234042553\n",
    "# batch_size=512 + слой dilation=2, padding=2       Accuracy: 0.8661170212765957\n",
    "# batch_size=512 + torch.optim.Adam                 Accuracy: 0.8668617021276596\n",
    "# batch_size=256 + torch.optim.Adam + Normalize     Accuracy: 0.8698404255319149\n",
    "# batch_size=256 + torch.optim.Adam + Normalize + Linear Accuracy: 0.8707446808510638!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703e227-fadd-4631-9df7-f9b52f517fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем директорию для сохранения весов\n",
    "import os\n",
    "os.makedirs('checkpoints/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53594048-9b1f-4b49-966e-183a56fa52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoints/cnn.pth')# сохраняем веса модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d02f-58b9-4729-a439-c38cffbdea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/cnn.pth'))# прикручиваем к ней словарь с весами и смотрим метрики\n",
    "validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6aa71-40e2-4d1a-95da-fecfdd273cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#модель сохраняем в ф-л model.pkl\n",
    "with open(os.path.join('myapp', 'model.pkl'),'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3f5c3-f9d3-41bc-804e-6dafa0434908",
   "metadata": {},
   "source": [
    "Вы можете заметить, что рукописные символы распознаются хуже, чем картинки из датасета. При желании попробуйте приблизить их к тем, на которых училась ваша модель. Возможно, вам пригодятся фильтры: Eroding and Dilating и Smoothing Images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
